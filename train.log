Using cuda.
48
total_rows: 34726
train_size: 26044
validation_size: 8681
Training Shape: (26044, 2)
Validation Shape: (8682, 2)
   Temperature   Outflow
0     3.698335  4.182050
1     3.698335  4.155753
2     3.698335  4.147885
3     3.698335  4.144721
4     3.698335  4.144721
Training Shape: (25949, 72, 2) (25949, 24, 2)
Validation Shape: (8587, 72, 2) (8587, 24, 2)
Type of X: <class 'numpy.ndarray'>
Type of y: <class 'numpy.ndarray'>
Type of X: <class 'numpy.ndarray'>
Type of y: <class 'numpy.ndarray'>
Features shape: torch.Size([64, 72, 2])
Target shape: torch.Size([64, 24, 2])
Net(
  (TCN): TCN(
    (network): Sequential(
      (0): TemporalBlock(
        (conv1): Conv1d(72, 72, kernel_size=(2,), stride=(1,), padding=(1,))
        (chomp1): Chomp1d()
        (relu1): ReLU()
        (dropout1): Dropout(p=0.2, inplace=False)
        (filter_conv2): Conv1d(72, 72, kernel_size=(2,), stride=(1,), padding=(1,))
        (gate_conv2): Conv1d(72, 72, kernel_size=(2,), stride=(1,), padding=(1,))
        (chomp2): Chomp1d()
        (dropout2): Dropout(p=0.2, inplace=False)
        (relu): ReLU()
      )
      (1): TemporalBlock(
        (conv1): Conv1d(72, 48, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))
        (chomp1): Chomp1d()
        (relu1): ReLU()
        (dropout1): Dropout(p=0.2, inplace=False)
        (filter_conv2): Conv1d(48, 48, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))
        (gate_conv2): Conv1d(48, 48, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))
        (chomp2): Chomp1d()
        (dropout2): Dropout(p=0.2, inplace=False)
        (downsample): Conv1d(72, 48, kernel_size=(1,), stride=(1,))
        (relu): ReLU()
      )
      (2): TemporalBlock(
        (conv1): Conv1d(48, 24, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))
        (chomp1): Chomp1d()
        (relu1): ReLU()
        (dropout1): Dropout(p=0.2, inplace=False)
        (filter_conv2): Conv1d(24, 24, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))
        (gate_conv2): Conv1d(24, 24, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))
        (chomp2): Chomp1d()
        (dropout2): Dropout(p=0.2, inplace=False)
        (downsample): Conv1d(48, 24, kernel_size=(1,), stride=(1,))
        (relu): ReLU()
      )
    )
    (linear): Linear(in_features=24, out_features=24, bias=True)
  )
  (conv1): Conv1d(72, 64, kernel_size=(1,), stride=(1,))
  (relu): ReLU()
  (conv2): Conv1d(64, 24, kernel_size=(1,), stride=(1,))
  (mamba): Sequential(
    (0): Linear(in_features=2, out_features=16, bias=True)
    (1): Mamba(
      (layers): ModuleList(
        (0): ResidualBlock(
          (mixer): MambaBlock(
            (in_proj): Linear(in_features=16, out_features=64, bias=False)
            (conv1d): Conv1d(32, 32, kernel_size=(4,), stride=(1,), padding=(3,), groups=32)
            (x_proj): Linear(in_features=32, out_features=33, bias=False)
            (dt_proj): Linear(in_features=1, out_features=32, bias=True)
            (out_proj): Linear(in_features=32, out_features=16, bias=False)
          )
          (norm): RMSNorm()
        )
        (1): ResidualBlock(
          (mixer): MambaBlock(
            (in_proj): Linear(in_features=16, out_features=64, bias=False)
            (conv1d): Conv1d(32, 32, kernel_size=(4,), stride=(1,), padding=(3,), groups=32)
            (x_proj): Linear(in_features=32, out_features=33, bias=False)
            (dt_proj): Linear(in_features=1, out_features=32, bias=True)
            (out_proj): Linear(in_features=32, out_features=16, bias=False)
          )
          (norm): RMSNorm()
        )
      )
      (norm_f): RMSNorm()
    )
    (2): Tanh()
    (3): Linear(in_features=16, out_features=1, bias=True)
  )
)

Total parameters: 71049
Learnable parameters: 71049

Epoch [0/299]		Train loss: 2.834408 - Val loss: 0.107496

Epoch [1/299]		Train loss: 0.109148 - Val loss: 0.038893

Epoch [2/299]		Train loss: 0.037334 - Val loss: 0.029108

Epoch [3/299]		Train loss: 0.021824 - Val loss: 0.025090

Epoch [4/299]		Train loss: 0.016323 - Val loss: 0.025834

Epoch [5/299]		Train loss: 0.014585 - Val loss: 0.021748

Epoch [6/299]		Train loss: 0.014392 - Val loss: 0.021476

Epoch [7/299]		Train loss: 0.013966 - Val loss: 0.018796

Epoch [8/299]		Train loss: 0.013488 - Val loss: 0.022897

Epoch [9/299]		Train loss: 0.013339 - Val loss: 0.019042

Epoch [10/299]		Train loss: 0.013593 - Val loss: 0.018163

Epoch [11/299]		Train loss: 0.013007 - Val loss: 0.018380

Epoch [12/299]		Train loss: 0.012787 - Val loss: 0.017639

Epoch [13/299]		Train loss: 0.012465 - Val loss: 0.017342

Epoch [14/299]		Train loss: 0.013012 - Val loss: 0.016764

Epoch [15/299]		Train loss: 0.012017 - Val loss: 0.017602

Epoch [16/299]		Train loss: 0.011715 - Val loss: 0.017539

Epoch [17/299]		Train loss: 0.011504 - Val loss: 0.015878

Epoch [18/299]		Train loss: 0.010738 - Val loss: 0.016616

Epoch [19/299]		Train loss: 0.010656 - Val loss: 0.020648

Epoch [20/299]		Train loss: 0.010438 - Val loss: 0.013891

Epoch [21/299]		Train loss: 0.010066 - Val loss: 0.016732

Epoch [22/299]		Train loss: 0.009335 - Val loss: 0.012769

Epoch [23/299]		Train loss: 0.009808 - Val loss: 0.013821

Epoch [24/299]		Train loss: 0.009303 - Val loss: 0.012136

Epoch [25/299]		Train loss: 0.009011 - Val loss: 0.013063

Epoch [26/299]		Train loss: 0.009224 - Val loss: 0.012009

Epoch [27/299]		Train loss: 0.009081 - Val loss: 0.012021

Epoch [28/299]		Train loss: 0.009002 - Val loss: 0.011725

Epoch [29/299]		Train loss: 0.008779 - Val loss: 0.014583

Epoch [30/299]		Train loss: 0.008888 - Val loss: 0.011891

Epoch [31/299]		Train loss: 0.008581 - Val loss: 0.012522

Epoch [32/299]		Train loss: 0.008632 - Val loss: 0.015910

Epoch [33/299]		Train loss: 0.008982 - Val loss: 0.012950

Epoch [34/299]		Train loss: 0.008518 - Val loss: 0.011660

Epoch [35/299]		Train loss: 0.008451 - Val loss: 0.011484

Epoch [36/299]		Train loss: 0.008474 - Val loss: 0.011182

Epoch [37/299]		Train loss: 0.008338 - Val loss: 0.011682

Epoch [38/299]		Train loss: 0.008368 - Val loss: 0.011712

Epoch [39/299]		Train loss: 0.008449 - Val loss: 0.011236

Epoch [40/299]		Train loss: 0.008214 - Val loss: 0.011574

Epoch [41/299]		Train loss: 0.008063 - Val loss: 0.011525

Epoch [42/299]		Train loss: 0.007989 - Val loss: 0.012705

Epoch [43/299]		Train loss: 0.008082 - Val loss: 0.010967

Epoch [44/299]		Train loss: 0.008271 - Val loss: 0.010819

Epoch [45/299]		Train loss: 0.007808 - Val loss: 0.010626

Epoch [46/299]		Train loss: 0.008246 - Val loss: 0.012298

Epoch [47/299]		Train loss: 0.008002 - Val loss: 0.011601

Epoch [48/299]		Train loss: 0.008110 - Val loss: 0.010207

Epoch [49/299]		Train loss: 0.007909 - Val loss: 0.010377

Epoch [50/299]		Train loss: 0.007583 - Val loss: 0.010223

Epoch [51/299]		Train loss: 0.007756 - Val loss: 0.011131

Epoch [52/299]		Train loss: 0.007493 - Val loss: 0.010845

Epoch [53/299]		Train loss: 0.007766 - Val loss: 0.011115

Epoch [54/299]		Train loss: 0.008107 - Val loss: 0.011269

Epoch [55/299]		Train loss: 0.007542 - Val loss: 0.009996

Epoch [56/299]		Train loss: 0.007497 - Val loss: 0.010986

Epoch [57/299]		Train loss: 0.007652 - Val loss: 0.010331

Epoch [58/299]		Train loss: 0.007711 - Val loss: 0.009861

Epoch [59/299]		Train loss: 0.007534 - Val loss: 0.010515

Epoch [60/299]		Train loss: 0.007528 - Val loss: 0.009937

Epoch [61/299]		Train loss: 0.007410 - Val loss: 0.010987

Epoch [62/299]		Train loss: 0.007715 - Val loss: 0.010533

Epoch [63/299]		Train loss: 0.007453 - Val loss: 0.014359

Epoch [64/299]		Train loss: 0.007628 - Val loss: 0.009929

Epoch [65/299]		Train loss: 0.007414 - Val loss: 0.010091

Epoch [66/299]		Train loss: 0.007404 - Val loss: 0.011493

Epoch [67/299]		Train loss: 0.007816 - Val loss: 0.009914

Epoch [68/299]		Train loss: 0.007658 - Val loss: 0.010495

Epoch [69/299]		Train loss: 0.007364 - Val loss: 0.010731
Epoch 00070: reducing learning rate of group 0 to 3.0000e-04.

Epoch [70/299]		Train loss: 0.006654 - Val loss: 0.009781

Epoch [71/299]		Train loss: 0.006702 - Val loss: 0.009578

Epoch [72/299]		Train loss: 0.006700 - Val loss: 0.009690

Epoch [73/299]		Train loss: 0.006783 - Val loss: 0.010142

Epoch [74/299]		Train loss: 0.006748 - Val loss: 0.009498

Epoch [75/299]		Train loss: 0.006677 - Val loss: 0.009625

Epoch [76/299]		Train loss: 0.006698 - Val loss: 0.009723

Epoch [77/299]		Train loss: 0.006769 - Val loss: 0.009569

Epoch [78/299]		Train loss: 0.006631 - Val loss: 0.009819

Epoch [79/299]		Train loss: 0.006724 - Val loss: 0.009731

Epoch [80/299]		Train loss: 0.006679 - Val loss: 0.009903

Epoch [81/299]		Train loss: 0.006697 - Val loss: 0.009685

Epoch [82/299]		Train loss: 0.006673 - Val loss: 0.009544

Epoch [83/299]		Train loss: 0.006667 - Val loss: 0.009925

Epoch [84/299]		Train loss: 0.006629 - Val loss: 0.010010

Epoch [85/299]		Train loss: 0.006689 - Val loss: 0.009655
Epoch 00086: reducing learning rate of group 0 to 9.0000e-05.

Epoch [86/299]		Train loss: 0.006422 - Val loss: 0.009613

Epoch [87/299]		Train loss: 0.006428 - Val loss: 0.009442

Epoch [88/299]		Train loss: 0.006449 - Val loss: 0.009719

Epoch [89/299]		Train loss: 0.006505 - Val loss: 0.009573

Epoch [90/299]		Train loss: 0.006453 - Val loss: 0.009486

Epoch [91/299]		Train loss: 0.006443 - Val loss: 0.009408

Epoch [92/299]		Train loss: 0.006425 - Val loss: 0.009609

Epoch [93/299]		Train loss: 0.006452 - Val loss: 0.009427

Epoch [94/299]		Train loss: 0.006500 - Val loss: 0.009606

Epoch [95/299]		Train loss: 0.006452 - Val loss: 0.009447

Epoch [96/299]		Train loss: 0.006444 - Val loss: 0.009451

Epoch [97/299]		Train loss: 0.006426 - Val loss: 0.009552

Epoch [98/299]		Train loss: 0.006453 - Val loss: 0.009754

Epoch [99/299]		Train loss: 0.006432 - Val loss: 0.009541

Epoch [100/299]		Train loss: 0.006419 - Val loss: 0.009449

Epoch [101/299]		Train loss: 0.006444 - Val loss: 0.009433

Epoch [102/299]		Train loss: 0.006445 - Val loss: 0.009429
Epoch 00103: reducing learning rate of group 0 to 2.7000e-05.

Epoch [103/299]		Train loss: 0.006347 - Val loss: 0.009453

Epoch [104/299]		Train loss: 0.006350 - Val loss: 0.009460

Epoch [105/299]		Train loss: 0.006350 - Val loss: 0.009398

Epoch [106/299]		Train loss: 0.006353 - Val loss: 0.009393

Epoch [107/299]		Train loss: 0.006351 - Val loss: 0.009388

Epoch [108/299]		Train loss: 0.006353 - Val loss: 0.009492

Epoch [109/299]		Train loss: 0.006353 - Val loss: 0.009396

Epoch [110/299]		Train loss: 0.006369 - Val loss: 0.009390

Epoch [111/299]		Train loss: 0.006353 - Val loss: 0.009398

Epoch [112/299]		Train loss: 0.006348 - Val loss: 0.009394

Epoch [113/299]		Train loss: 0.006346 - Val loss: 0.009480

Epoch [114/299]		Train loss: 0.006348 - Val loss: 0.009402

Epoch [115/299]		Train loss: 0.006350 - Val loss: 0.009523

Epoch [116/299]		Train loss: 0.006358 - Val loss: 0.009414

Epoch [117/299]		Train loss: 0.006342 - Val loss: 0.009461

Epoch [118/299]		Train loss: 0.006349 - Val loss: 0.009392
Epoch 00119: reducing learning rate of group 0 to 8.1000e-06.

Epoch [119/299]		Train loss: 0.006314 - Val loss: 0.009375

Epoch [120/299]		Train loss: 0.006319 - Val loss: 0.009402

Epoch [121/299]		Train loss: 0.006319 - Val loss: 0.009460

Epoch [122/299]		Train loss: 0.006314 - Val loss: 0.009435

Epoch [123/299]		Train loss: 0.006312 - Val loss: 0.009380

Epoch [124/299]		Train loss: 0.006321 - Val loss: 0.009404

Epoch [125/299]		Train loss: 0.006310 - Val loss: 0.009390

Epoch [126/299]		Train loss: 0.006313 - Val loss: 0.009407

Epoch [127/299]		Train loss: 0.006320 - Val loss: 0.009395

Epoch [128/299]		Train loss: 0.006315 - Val loss: 0.009395

Epoch [129/299]		Train loss: 0.006312 - Val loss: 0.009423

Epoch [130/299]		Train loss: 0.006314 - Val loss: 0.009379
Epoch 00131: reducing learning rate of group 0 to 2.4300e-06.

Epoch [131/299]		Train loss: 0.006309 - Val loss: 0.009393

Epoch [132/299]		Train loss: 0.006303 - Val loss: 0.009391

Epoch [133/299]		Train loss: 0.006303 - Val loss: 0.009413

Epoch [134/299]		Train loss: 0.006305 - Val loss: 0.009376

Epoch [135/299]		Train loss: 0.006304 - Val loss: 0.009383

Epoch [136/299]		Train loss: 0.006302 - Val loss: 0.009372

Epoch [137/299]		Train loss: 0.006303 - Val loss: 0.009378

Epoch [138/299]		Train loss: 0.006305 - Val loss: 0.009371

Epoch [139/299]		Train loss: 0.006298 - Val loss: 0.009423

Epoch [140/299]		Train loss: 0.006306 - Val loss: 0.009371

Epoch [141/299]		Train loss: 0.006310 - Val loss: 0.009398

Epoch [142/299]		Train loss: 0.006305 - Val loss: 0.009427

Epoch [143/299]		Train loss: 0.006302 - Val loss: 0.009401

Epoch [144/299]		Train loss: 0.006300 - Val loss: 0.009380

Epoch [145/299]		Train loss: 0.006300 - Val loss: 0.009419

Epoch [146/299]		Train loss: 0.006304 - Val loss: 0.009392

Epoch [147/299]		Train loss: 0.006302 - Val loss: 0.009384
Epoch 00148: reducing learning rate of group 0 to 7.2900e-07.

Epoch [148/299]		Train loss: 0.006308 - Val loss: 0.009382

Epoch [149/299]		Train loss: 0.006301 - Val loss: 0.009383

Epoch [150/299]		Train loss: 0.006298 - Val loss: 0.009383

Epoch [151/299]		Train loss: 0.006298 - Val loss: 0.009387

Epoch [152/299]		Train loss: 0.006299 - Val loss: 0.009385

Epoch [153/299]		Train loss: 0.006294 - Val loss: 0.009379

Epoch [154/299]		Train loss: 0.006300 - Val loss: 0.009396

Epoch [155/299]		Train loss: 0.006298 - Val loss: 0.009400

Epoch [156/299]		Train loss: 0.006300 - Val loss: 0.009386

Epoch [157/299]		Train loss: 0.006298 - Val loss: 0.009386

Epoch [158/299]		Train loss: 0.006298 - Val loss: 0.009385
Epoch 00159: reducing learning rate of group 0 to 2.1870e-07.

Epoch [159/299]		Train loss: 0.006300 - Val loss: 0.009386

Epoch [160/299]		Train loss: 0.006301 - Val loss: 0.009382
Early Stopping.
Save best model at epoch 140
[[4.6291213]
 [4.5848365]
 [4.563495 ]
 [4.5207334]
 [4.4775643]
 [4.404401 ]
 [4.338961 ]
 [4.2960224]
 [4.2588496]
 [4.249837 ]
 [4.2709846]
 [4.294932 ]
 [4.3269963]
 [4.375995 ]
 [4.408392 ]
 [4.420481 ]
 [4.4287863]
 [4.4239664]
 [4.4305468]
 [4.4513717]
 [4.4701657]
 [4.4926763]
 [4.5199375]
 [4.5399003]]
y_pred Shape: (8587, 24, 1)
y_pred Shape: (8587, 24, 1)
y_test Shape: (8587, 24)
(8587, 24)
Overall forecast MAE : 5.0942
Overall forecast MSE: 6.7413
Overall forecast NSE: 0.9181
Overall forecast R^2: 0.9183
Overall forecast RSR: 0.2861
Overall forecast Pbias: -0.64%
(8587, 24, 2)
(8587, 24, 1)

Process finished with exit code 1
